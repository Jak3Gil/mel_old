# ğŸ¤ Melvin Audio Subsystem

**Convert sound waves into knowledge**

The audio subsystem enables Melvin to hear, understand, and learn from speech and environmental sounds, integrating auditory perception with vision and reasoning.

---

## ğŸš€ Quick Start

### Build the Demo

```bash
make -f Makefile.audio demo
./demo_audio_integration
```

### Run Tests

```bash
make -f Makefile.audio test
./test_audio_bridge
```

### Complete Build & Test

```bash
make -f Makefile.audio complete
```

---

## ğŸ“ File Structure

```
melvin/
â”œâ”€â”€ audio/
â”‚   â”œâ”€â”€ audio_pipeline.h          # Audio capture & processing
â”‚   â”œâ”€â”€ audio_pipeline.cpp
â”‚   â”œâ”€â”€ audio_bridge.h             # Graph integration
â”‚   â””â”€â”€ audio_bridge.cpp
â”œâ”€â”€ demos/
â”‚   â””â”€â”€ demo_audio_integration.cpp # Full demo with examples
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_audio_bridge.cpp      # Test suite
â””â”€â”€ scripts/
    â”œâ”€â”€ recognize_speech.py        # Whisper integration
    â””â”€â”€ classify_sound.py          # Ambient sound classification

docs/
â””â”€â”€ architecture_audio.md          # Full documentation

Makefile.audio                     # Build system
```

---

## ğŸ¯ Features

### AudioPipeline
- âœ… Microphone capture (live streaming)
- âœ… Voice Activity Detection (VAD)
- âœ… Speech recognition (Whisper/Vosk ready)
- âœ… Ambient sound classification (YAMNet ready)
- âœ… Rolling audio buffer
- âœ… Event-based output

### AudioBridge
- âœ… Speech â†’ Knowledge graph nodes
- âœ… Word-level tokenization
- âœ… Cross-modal sync (audio â†” vision)
- âœ… Temporal windowing (co-occurrence)
- âœ… Causal inference
- âœ… Edge reinforcement & decay

---

## ğŸ”§ Integration

### Basic Usage

```cpp
#include "melvin/audio/audio_pipeline.h"
#include "melvin/audio/audio_bridge.h"
#include "melvin/core/atomic_graph.h"

int main() {
    AtomicGraph graph;
    AudioPipeline audio;
    AudioBridge bridge;
    
    // Start listening
    audio.start_stream();
    
    // Main loop
    while (running) {
        audio.tick(dt);
        auto events = audio.get_recent_events();
        
        for (const auto& event : events) {
            bridge.process(event, graph);
        }
    }
    
    // Save knowledge
    graph.save("nodes.bin", "edges.bin");
    return 0;
}
```

### With Cross-Modal Integration

```cpp
// Your perception loop
void perception_loop(float dt) {
    // Get events from all modalities
    auto audio_events = audio_pipeline.get_recent_events();
    auto visual_events = vision_pipeline.get_recent_events();
    
    // Process audio
    for (const auto& event : audio_events) {
        audio_bridge.process(event, graph);
    }
    
    // Synchronize across modalities
    audio_bridge.sync_with_vision(audio_events, visual_events, graph);
}
```

---

## ğŸ§ª Demo Scenarios

The demo includes 4 scenarios:

### 1. Basic Audio Capture
- Process speech and ambient sounds
- Create graph nodes and edges
- Display statistics

### 2. Cross-Modal Integration
- Combine audio with vision
- Create temporal associations
- Demonstrate co-occurrence learning

### 3. Reflection Mode
- Load previous session
- Analyze learned patterns
- Show knowledge persistence

### 4. Continuous Learning
- Process multiple events
- Apply edge decay
- Build knowledge over time

**Run all demos:**
```bash
./demo_audio_integration 5
```

---

## ğŸ“Š Example Output

```
ğŸ¤ Speech processed: "turn on the stove" (node 1)
   - Created phrase node: audio:turn on the stove
   - Created word nodes: turn, on, the, stove
   - Created 5 edges (temporal sequence)

ğŸ‘ï¸  Vision: Detected 'stove' at t=1.1s

ğŸ”— Cross-modal link: "turn on the stove" â†” "stove"

ğŸ“Š AudioBridge Statistics:
   Events processed: 1
   Nodes created: 6
   Edges created: 6
   Cross-modal links: 1
```

---

## ğŸ”¬ Test Suite

8 comprehensive tests:

1. âœ… Basic audio event processing
2. âœ… Word tokenization
3. âœ… Cross-modal synchronization
4. âœ… Temporal window filtering
5. âœ… Confidence threshold filtering
6. âœ… Edge reinforcement and decay
7. âœ… Ambient sound processing
8. âœ… Graph persistence

**Run tests:**
```bash
./test_audio_bridge
```

---

## âš™ï¸ Configuration

### AudioPipeline Config

```cpp
AudioPipeline::Config config;
config.sample_rate = 16000;         // Hz (Whisper uses 16kHz)
config.channels = 1;                // Mono
config.buffer_size_ms = 3000;       // 3-second buffer
config.vad_threshold = 0.02f;       // Voice activity threshold
config.enable_speech = true;
config.enable_ambient = true;
config.recognition_engine = "whisper";

AudioPipeline audio(config);
```

### AudioBridge Config

```cpp
AudioBridge::Config config;
config.temporal_window = 3.0f;      // 3s co-occurrence window
config.min_confidence = 0.3f;       // Filter low-confidence events
config.create_word_nodes = true;
config.enable_cross_modal = true;
config.enable_causal_inference = true;

AudioBridge bridge(config);
```

---

## ğŸ”Œ Speech Recognition Integration

### Using Whisper (Recommended)

Install Whisper:
```bash
pip install openai-whisper
```

The pipeline calls `scripts/recognize_speech.py`:
```python
import whisper
import sys

model = whisper.load_model("base")
audio_path = sys.argv[1]

result = model.transcribe(audio_path)
print(result["text"])
```

### Using Vosk (Lightweight)

Install Vosk:
```bash
pip install vosk
```

Download model and configure in `audio_pipeline.cpp`.

---

## ğŸŒ Ambient Sound Classification

### Using YAMNet

Install TensorFlow:
```bash
pip install tensorflow tensorflow-hub
```

The pipeline calls `scripts/classify_sound.py`:
```python
import tensorflow_hub as hub
import numpy as np

model = hub.load('https://tfhub.dev/google/yamnet/1')
# Process audio and return classification
```

---

## ğŸ“ How It Works

### 1. Sound â†’ Event

```
Audio Input â†’ VAD â†’ Recognition â†’ AudioEvent
  16kHz PCM    âœ“      "hello"      {label, time, conf}
```

### 2. Event â†’ Graph Nodes

```
AudioEvent("turn on the stove")
    â†“
Nodes:
  - audio:turn on the stove [PHRASE]
  - audio:turn [WORD]
  - audio:on [WORD]
  - audio:stove [WORD]
  - audio:speech [CATEGORY]
```

### 3. Cross-Modal Linking

```
Speech (t=0.5s): "stove"
Vision (t=1.1s): stove detected
    â†“
Edge: audio:stove â†” vision:stove [CO_OCCURS_WITH]
```

### 4. Persistence

```
graph.save("nodes.bin", "edges.bin")
    â†“
All audio knowledge preserved!
```

---

## ğŸ“š Full Documentation

See `docs/architecture_audio.md` for:
- Complete architecture overview
- Detailed component descriptions
- Event flow diagrams
- Integration examples
- Performance characteristics
- Configuration guide
- Future extensions

---

## ğŸ› ï¸ Troubleshooting

### No audio events generated
- Check microphone permissions
- Verify `audio.is_running()` returns true
- Lower `vad_threshold` for quiet environments

### Speech not recognized
- Install Whisper: `pip install openai-whisper`
- Ensure Python scripts are executable
- Check `scripts/recognize_speech.py` exists

### Cross-modal links not created
- Check `temporal_window` is appropriate
- Verify events have similar timestamps
- Ensure `config.enable_cross_modal = true`

### Build errors
- Ensure C++17 compiler available
- Check `atomic_graph.cpp` is compiled
- Run `make -f Makefile.audio clean` first

---

## ğŸš§ Development Status

### âœ… Implemented
- Core audio pipeline architecture
- Event processing system
- Graph integration
- Cross-modal synchronization
- Test suite
- Demo applications
- Documentation

### ğŸ”œ Coming Soon
- Live microphone capture (PortAudio)
- Actual Whisper integration
- YAMNet ambient classification
- Speaker identification
- Emotional tone detection
- Sound localization (stereo)

---

## ğŸ“ Example Scenarios

### Scenario 1: Kitchen Assistant

```
User: "Turn on the stove"
  â†’ Audio: Creates speech nodes
  â†’ Vision: Detects stove
  â†’ Link: audio:stove â†” vision:stove
  â†’ Action: Melvin moves to stove

Result: Melvin learned "turn on stove" â†’ move_to(stove)
```

### Scenario 2: Environmental Awareness

```
Ambient: "dog barking" (t=1.0s)
Vision: dog detected (t=1.2s)
  â†’ Link: audio:dog barking â†” vision:dog
  
Ambient: "door closing" (t=5.0s)
Vision: door detected (t=5.1s)
  â†’ Link: audio:door closing â†” vision:door

Result: Melvin associates sounds with visual objects
```

---

## ğŸ¤ Contributing

To extend the audio subsystem:

1. Add new event types to `AudioEvent`
2. Extend node types in `audio_bridge.h`
3. Implement recognition in `audio_pipeline.cpp`
4. Add tests in `test_audio_bridge.cpp`
5. Update documentation

---

## ğŸ“„ License

Part of the Melvin AI project.

---

## ğŸ™ Acknowledgments

- OpenAI Whisper for speech recognition
- Google YAMNet for audio classification
- PortAudio for cross-platform audio I/O

---

**Ready to give Melvin ears!** ğŸ§

For questions or issues, see `docs/architecture_audio.md` or create an issue.

