# üß† MELVIN LEARNING MODES

## Choose Your Knowledge Source!

---

## ü•á **RECOMMENDED: Ollama Mode (AI-Powered)**

### What It Is:
Uses **local Llama 3.2** to generate high-quality educational facts

### Quality:
- ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **EXCELLENT** - Real educational content
- Scientifically accurate
- Grammatically perfect
- Diverse topics

### Speed:
- ~0.2 facts/sec generation
- ~50,200 facts/sec ingestion
- **Overall: Fast enough!**

### Requirements:
```bash
# Install Ollama
brew install ollama  # or download from ollama.ai

# Start Ollama
ollama serve

# Pull Llama 3.2
ollama pull llama3.2
```

### Run:
```bash
./run_ollama_hypergrowth.sh
```

### Sample Facts:
```
"In quantum mechanics, particles can exhibit wave-particle duality..."
"The process of mitosis allows for the precise replication of genetic material..."
"Water molecules are polar, meaning they have a slightly positive charge..."
```

**PROS:**
- ‚úÖ 100% local (no internet needed)
- ‚úÖ Highest quality facts
- ‚úÖ Unlimited generation
- ‚úÖ Free!
- ‚úÖ Privacy-preserving

**CONS:**
- ‚ö†Ô∏è Requires Ollama installed
- ‚ö†Ô∏è Slower generation (but worth it!)

---

## üåê **Wikipedia Mode (Real Web Data)**

### What It Is:
Scrapes **real Wikipedia articles** via API + some synthetic fill

### Quality:
- ‚≠ê‚≠ê‚≠ê‚≠ê **VERY GOOD** - Mix of real + synthetic
- ~26% real Wikipedia
- ~74% synthetic (educational)

### Speed:
- **SUPER FAST!**
- ~50,200 facts/sec processing
- No waiting for generation

### Requirements:
- Internet connection
- Nothing else!

### Run:
```bash
./run_hypergrowth.sh  # Old hypergrowth (Wikipedia)
```

### Sample Facts:
```
"Ruien Xiao is a Canadian fencer..."  (REAL Wikipedia)
"MIT Technology Review is a bimonthly magazine..."  (REAL Wikipedia)
"cellular respiration determines biological processes"  (Synthetic)
```

**PROS:**
- ‚úÖ No local AI needed
- ‚úÖ Real Wikipedia data
- ‚úÖ Very fast
- ‚úÖ Free API

**CONS:**
- ‚ö†Ô∏è Needs internet
- ‚ö†Ô∏è Only ~26% real facts
- ‚ö†Ô∏è Rest is synthetic

---

## üéØ **Normal Mode (Balanced)**

### What It Is:
100 facts per cycle from Wikipedia

### Quality:
- ‚≠ê‚≠ê‚≠ê **GOOD** - Mix of sources

### Speed:
- **FAST**
- ~2,400 facts/sec processing

### Requirements:
- Internet connection

### Run:
```bash
./run_continuous.sh
```

**PROS:**
- ‚úÖ Balanced approach
- ‚úÖ Less aggressive

**CONS:**
- ‚ö†Ô∏è Slower growth than hypergrowth

---

## üìä **Comparison Table**

| Mode | Quality | Speed | Local | Internet | Growth Rate |
|------|---------|-------|-------|----------|-------------|
| **Ollama** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Medium | ‚úÖ | ‚ùå | High |
| **Wikipedia** | ‚≠ê‚≠ê‚≠ê‚≠ê | Fast | ‚ùå | ‚úÖ | Very High |
| **Normal** | ‚≠ê‚≠ê‚≠ê | Fast | ‚ùå | ‚úÖ | Medium |

---

## üöÄ **Recommended Setup**

### For BEST Quality:
```bash
# 1. Install Ollama
ollama serve

# 2. Pull Llama 3.2
ollama pull llama3.2

# 3. Run Ollama mode
./run_ollama_hypergrowth.sh
```

### For FASTEST Growth:
```bash
# Use Wikipedia hypergrowth
./run_hypergrowth.sh
```

### For BALANCED:
```bash
# Use normal mode
./run_continuous.sh
```

---

## üéÆ **Quick Commands**

### Start Ollama Mode (RECOMMENDED!)
```bash
# Make sure Ollama is running first!
ollama serve  # In one terminal

# Then start learning
./run_ollama_hypergrowth.sh
```

### Test Ollama Generator
```bash
python3 ollama_scraper.py 50
```

### Check What's Running
```bash
ps aux | grep -E "run_.*hypergrowth|run_continuous"
```

### Stop All Learning
```bash
pkill -f "run_continuous|run_hypergrowth|run_ollama"
```

---

## üí° **My Recommendation**

**Use Ollama Mode!** üèÜ

Why?
1. **Highest quality** - Real educational facts
2. **100% local** - No internet needed
3. **Unlimited** - Generate as much as you want
4. **Free** - No API costs
5. **Privacy** - All local processing

The only downside is it's slower (~3 minutes per 100 facts), but the quality is **SO MUCH BETTER** that it's worth it!

---

## üîß **Troubleshooting**

### Ollama Not Running:
```bash
# Start Ollama
ollama serve

# Pull model
ollama pull llama3.2

# Test
curl http://localhost:11434/api/tags
```

### Change Ollama Model:
Edit `ollama_scraper.py`:
```python
OllamaScraper(model="llama3.1")  # or llama2, mistral, etc.
```

### Speed Up Ollama:
```python
# In ollama_scraper.py, reduce facts per topic:
facts_per_topic = 2  # Instead of 3
```

---

## üéâ **Bottom Line**

**Ollama Mode = Best Quality**  
**Wikipedia Mode = Fastest Growth**  
**Normal Mode = Balanced**

**Try Ollama first - you'll love the quality!** üöÄ

