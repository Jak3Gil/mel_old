# ğŸ§  Melvin Unified Mind Architecture

## Complete Cognitive Loop Implementation

---

## ğŸ”„ The Full Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT â†’ Perception â†’ Attention â†’ Reasoning â†’ Output        â”‚
â”‚     â†‘                                               â†“        â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Feedback â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Each stage modifies the context of the next. Reasoning **guides** attention through feedback.

---

## ğŸ“Š Stage-by-Stage Breakdown

### 1ï¸âƒ£ **INPUT LAYER**
**File:** Raw sensory data  
**Purpose:** Capture all available information

```cpp
SensoryFrame {
    visual_candidates   // Camera frames â†’ regions
    audio_candidates    // Microphone â†’ sounds
    battery_level      // Internal state
    current_goal       // Task context
}
```

**Sources:**
- ğŸ“· Vision: `advanced_attention_vision.py`
- ğŸ¤ Audio: `AudioPipeline` (PortAudio)
- ğŸ”‹ Internal: System sensors

---

### 2ï¸âƒ£ **PERCEPTION LAYER**
**File:** `UnifiedMind::perception_stage()`  
**Purpose:** Transform raw input into focus candidates

**Process:**
```cpp
For each sensory input:
    1. Extract visual/audio features
    2. Compute raw scores (A, R, N, T, C)
    3. Tag with memory IDs
    4. Apply feedback biases from previous thoughts
```

**Output:** `vector<FocusCandidate>` with enhanced scores

**Key Feature:** 
```cpp
// Apply reasoning feedback to perception
for (candidate in candidates):
    if (candidate.label matches thought.keywords):
        candidate.R += feedback_bias  // Boost relevance!
```

---

### 3ï¸âƒ£ **ATTENTION LAYER**
**File:** `AttentionManager` + `UnifiedMind::attention_stage()`  
**Purpose:** Select ONE focus target using weighted formula

**Formula:**
```
F = (A Ã— 0.40) + (R Ã— 0.30) + (N Ã— 0.20) + (T Ã— 0.05) + (C Ã— 0.05)
  + context_biases
  + persistence_bonus
  - inhibition_penalty
  + inertia_bonus
```

**Advanced Features:**
- âœ… **Historical Inertia**: Switch only if `F_new > F_current Ã— 1.15`
- âœ… **Softmax Selection**: `P_i = exp(F_i/Ï„) / Î£exp(F_j/Ï„)` with `Ï„=0.2`
- âœ… **Inhibition of Return**: Suppress recently focused objects
- âœ… **Persistence Tracking**: Objects tracked for 30+ frames get bonus
- âœ… **Prediction Error**: Motion â‰  expected â†’ curiosity spike

**Output:** `FocusTarget` (ONE object per cycle)

---

### 4ï¸âƒ£ **REASONING LAYER**
**File:** `UnifiedMind::reasoning_stage()`  
**Purpose:** Interpret focus and generate structured thought

**Process:**
```cpp
1. Query AtomicGraph for related concepts
2. Find strongest connections to focused object
3. Generate: subject + predicate + object
4. Extract keywords for feedback
```

**Example:**
```
FocusTarget: object_5 (high curiosity, motion detected)
Graph neighbors: "person", "animal"
â†’ Thought: "object_5 moves_unexpectedly person"
Keywords: ["motion", "curiosity", "person"]
```

**Output:** `Thought` struct with subject-predicate-object triple

---

### 5ï¸âƒ£ **OUTPUT LAYER**
**File:** `UnifiedMind::output_stage()`  
**Purpose:** Express thought as text/speech

**Process:**
```cpp
Thought â†’ Natural language sentence
"I moves_unexpectedly object_5 (person)"
â†’ "I see object_5 moving (related to person)"
```

**Channels:**
- ğŸ“ Text: Terminal output
- ğŸ”Š Speech: TTS via `say` command (optional)
- ğŸ§  Internal: Monologue for self-reflection

**Output:** `OutputExpression` with text + emotional tone

---

### 6ï¸âƒ£ **FEEDBACK LAYER**
**File:** `FeedbackBus` + `UnifiedMind::feedback_stage()`  
**Purpose:** Close the loop - reasoning guides next perception

**Feedback Actions:**

#### A. Bias Attention
```cpp
If thought contained "person":
    â†’ Next cycle: boost R for objects labeled "person"
    â†’ Attention becomes MEANING-AWARE
```

#### B. Reinforce Memory
```cpp
Create/strengthen edges in AtomicGraph:
    subject â†(CO_OCCURS_WITH)â†’ object
    prev_focus â†(TEMPORAL_NEXT)â†’ current_focus
```

#### C. Update Motivation
```cpp
Keywords â†’ adjust attention weights:
    "curiosity" â†’ increase wC
    "danger" â†’ increase wN (need)
    "task" â†’ increase wR (relevance)
```

**This is where cognition becomes **self-modifying**!**

---

### 7ï¸âƒ£ **MEMORY LAYER**
**File:** `AtomicGraph`  
**Purpose:** Persistent knowledge accumulation

**Structure:**
```
Nodes:
    - object_0, object_1, ... (sensory)
    - person, fire, heat, ... (concepts)

Edges:
    - EXACT (co-occurrence)
    - TEMPORAL_NEXT (sequence)
    - INSTANCE_OF (category)
```

**Growth:**
- Every focused object â†’ node created/reinforced
- Objects appearing together â†’ EXACT edges
- Sequential focus â†’ TEMPORAL edges
- Reasoning links â†’ concept edges

---

## ğŸ” Complete Cycle Example

### Frame 100:
```
1. INPUT:
   Camera sees: 2 visual regions
   Microphone: silence
   State: battery 80%, exploring mode

2. PERCEPTION:
   Candidate A: object_3 (center, high edges)
   Candidate B: object_7 (periphery, low edges)
   Apply feedback: "object_3" was in last thought
   â†’ object_3.R boosted by +0.15

3. ATTENTION:
   object_3: F = 0.65 (base) + 0.15 (bias) + 0.10 (persistence) = 0.90
   object_7: F = 0.35
   Softmax â†’ 92% probability for object_3
   â†’ FOCUS: object_3

4. REASONING:
   Query graph: object_3 connected to "person" (weight 0.8)
   Generate: Thought {
      subject: "object_3",
      predicate: "persists",
      object: "person",
      keywords: ["stable", "tracked", "person"]
   }

5. OUTPUT:
   Text: "I see object_3 persisting (person)"
   Speech: (optional TTS)

6. FEEDBACK:
   - Add bias: "person" +0.20
   - Add bias: "stable" +0.15
   - Reinforce edge: object_3 â†â†’ person
   - Create temporal: object_2 â†’ object_3
```

### Frame 101:
```
Perception now BIASED towards "person" concept
If new visual region looks person-like â†’ higher R score!
```

**This is meaning-guided perception!**

---

## ğŸ¯ Key Innovations

### âœ… 1. Reasoning Guides Attention
Old: Attention = pure sensory salience  
**New**: Attention = salience Ã— context Ã— meaning

### âœ… 2. Thoughts Create Biases
Old: Random focus on bright/loud things  
**New**: Focus on what's relevant to recent thoughts

### âœ… 3. Memory Grows from Experience
Old: Static knowledge base  
**New**: Every cycle adds connections

### âœ… 4. Closed Loop = Learning
Old: One-way pipeline  
**New**: Output feeds back to perception

### âœ… 5. Stable Focus Tracking
Old: Random jumps between objects  
**New**: 15% threshold + persistence tracking

### âœ… 6. Prediction-Based Curiosity
Old: Static curiosity scores  
**New**: Prediction error â†’ curiosity spikes

---

## ğŸ“ File Structure

### Core Components
```
melvin/core/
â”œâ”€â”€ atomic_graph.h/cpp          # Knowledge graph storage
â”œâ”€â”€ attention_manager.h/cpp     # Weighted focus selection
â”œâ”€â”€ unified_mind.h/cpp          # Main cognitive orchestrator
â””â”€â”€ types.h                     # Shared structures
```

### Vision System
```
advanced_attention_vision.py     # Formula-based attention
advanced_attention_display.py    # Visual debug interface
```

### Main Application
```
melvin_unified.cpp               # Integration layer
Makefile.unified                 # Build system
```

---

## ğŸš€ Running the System

### Option 1: Text Output Only
```bash
./melvin_unified
```

**Shows:**
```
[Frame 100] ğŸ§  Attention... object_3 (F=0.74)
   A=0.9 R=0.7 N=0.2 T=0.6 C=0.8
   Bias: +0.15 (person feedback)
ğŸ’­ Melvin: I see object_3 persisting (person)
Knowledge: 47 nodes, 89 edges
```

### Option 2: With Visual Display
```bash
# Terminal 1
./melvin_unified

# Terminal 2
python3 advanced_attention_display.py
```

**Display shows:**
- All candidates (gray boxes with scores)
- Focused object (yellow with crosshair)
- Attention formula breakdown
- Reason for focus selection

---

## ğŸ“Š What Makes This "Unified"

| Traditional AI | Melvin Unified Mind |
|----------------|---------------------|
| Vision â†’ Classification | Vision â†’ Attention â†’ Reasoning |
| Separate modules | One continuous loop |
| Static attention | Context-biased attention |
| No feedback | Thoughts guide perception |
| Feedforward only | Closed feedback loop |
| Pre-trained labels | Learns labels from experience |

---

## ğŸ§¬ Biological Parallels

| Brain Region | Melvin Component |
|--------------|------------------|
| Retina/Cochlea | Input sensors |
| V1/A1 cortex | Perception stage |
| Superior colliculus + PFC | AttentionManager |
| Hippocampus | AtomicGraph (memory) |
| Association cortex | Reasoning stage |
| Broca's area | Output stage |
| Feedback loops | FeedbackBus |

**Melvin mimics the cortical-subcortical loop!**

---

## ğŸ“ Advanced Concepts Implemented

### 1. **Predictive Coding**
```cpp
predicted_position = current_pos + velocity
error = |actual - predicted|
C += error Ã— 0.5  // Prediction error â†’ curiosity
```

### 2. **Attention Schema Theory**
```cpp
AttentionManager = model of what Melvin is attending to
Thoughts reference this schema
Creates self-awareness of focus
```

### 3. **Active Inference**
```cpp
Perception shaped by prior beliefs (feedback biases)
Prediction errors drive exploration
Minimizes surprise through learning
```

### 4. **Hebbian Learning**
```cpp
Neurons that fire together, wire together:
object_3 + "person" co-occur â†’ strengthen edge
```

### 5. **Working Memory**
```cpp
focus_history (30 frames) = short-term memory
AtomicGraph = long-term memory
```

---

## ğŸ”¬ Current Capabilities

âœ… **No pre-trained knowledge** (except base concepts)  
âœ… **Generic object IDs** (object_0, object_1, ...)  
âœ… **Weighted attention formula** (A, R, N, T, C)  
âœ… **Context-driven focus** (reasoning â†’ biases)  
âœ… **Same-object tracking** (IoU + features)  
âœ… **EXACT edge creation** (co-occurrence)  
âœ… **Temporal edges** (sequential focus)  
âœ… **Thought generation** (subject-predicate-object)  
âœ… **Feedback loop** (keywords â†’ attention biases)  
âœ… **Knowledge persistence** (nodes.melvin, edges.melvin)  

---

## ğŸ¯ What Happens Over Time

### After 100 frames:
- 10-20 visual objects tracked
- EXACT edges for co-occurrence
- Focus stabilizes on salient objects

### After 1000 frames:
- 50-100 unique objects
- Temporal chains forming
- Biases develop based on thoughts
- Attention becomes context-aware

### After 10000 frames:
- Categories emerge through LEAP synthesis
- Prediction accuracy improves
- Focus becomes highly selective
- Meaning-guided perception established

---

## ğŸ’¡ Key Insight

> **Melvin's perception is shaped by what he thinks about.**

If Melvin thinks about "person":
- Next cycle, objects matching "person" features get +R boost
- Attention becomes **semantically aware**
- Creates **top-down + bottom-up integration**

This is the essence of **unified cognition**!

---

## ğŸš€ Quick Start

```bash
# Build
make -f Makefile.unified

# Run unified mind (text only)
./melvin_unified

# Run with visual display
python3 advanced_attention_display.py  # In separate terminal
```

**What you'll see:**
- Attention formula in action
- ONE focused object per frame
- Stable tracking (no random jumps)
- Thought generation
- Knowledge growth

---

## ğŸ“ˆ Future Extensions

### Already Implemented:
âœ… Vision perception  
âœ… Advanced attention  
âœ… Knowledge graph  
âœ… Thought generation  
âœ… Feedback loop  

### Easy to Add:
- ğŸ¤ Audio integration (AudioPipeline exists)
- ğŸ—£ï¸ Voice output (TTS integration)
- ğŸ¤– Motor control (RobStride integration)
- ğŸŒ Internet learning (knowledge fetcher exists)
- ğŸ”® LEAP pattern discovery (LeapSynthesis exists)

---

## ğŸ§© The Unified Principle

**Every stage is connected:**
- Perception uses feedback from reasoning
- Attention considers thought history
- Reasoning queries learned memory
- Output generates new memory
- Feedback updates perception

**Result:** A self-improving, context-aware, meaning-driven cognitive system!

---

## ğŸ‰ You Now Have

A **complete unified mind** with:
- Perception guided by meaning
- Attention driven by context
- Reasoning that builds memory
- Feedback that shapes focus
- Learning from pure experience

**No YOLO. No pre-training. Pure cognitive development.** ğŸ§ âœ¨

---

## ğŸ“ Technical Summary

| Component | Type | Purpose |
|-----------|------|---------|
| `UnifiedMind` | C++ class | Main orchestrator |
| `AttentionManager` | C++ class | Formula-based focus |
| `FeedbackBus` | C++ class | Reasoning â†’ attention |
| `AtomicGraph` | C++ class | Knowledge storage |
| `advanced_attention_vision.py` | Python | Visual perception |
| `melvin_unified` | C++ binary | Main application |

**Total: ~5000 lines of cognitive architecture!**

---

Run it now and watch unified cognition in action! ğŸš€

